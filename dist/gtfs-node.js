/*eslint-env node*/
/*
Crude Node.js frontend for GTFS to Aquius.

Recommended when handling large (100+ MB) GTFS datasets.
Thereafter the limit is Node and ultimately System memory.
Try granting Node up to 10MB of memory per MB of GTFS text.
For example, for 30GB: node --max-old-space-size=30000

Setup is the same as the browser version: Extract the GTFS into a directory,
alongside any config.json and boundaries.geojson.
Other operations, such as unzipping the GTFS, or managing config files,
will need be done using other bespoke scripts.

Then command line: node path/to/gtfs-node.js path/to/data_directory/
Or with more memory: node --max-old-space-size=30000 path/to/gtfs-node.js path/to/data_directory/

Outputs added (created or overwritten) in the data directory:
- aquius.json - output for use in the wider Aquius ecosystem.
- config.auto - augmented or generated config.json,
    rename to config.json to reuse in future GTFS processing.
- histogram.csv - proprotion of services by hour of day, useful for quality assurance.
- network.csv - service totals by network, useful for quality assurance.
*/
const fs = require("fs");
const readline = require("readline");
const processing = require("./gtfs.js").gtfsToAquius.process;


async function init() {
    /**
     * Loads raw data from directory, initiates GTFS to Aquius processing,
     * and handles the result.
     * @return {string} Status once run complete.
     */
    
    let directory = process.argv.slice(2).join(" ");

    if (directory.length == 0) {
        return "Missing directory argument. Expected: " +
            "node path/to/gtfs-node.js path/to/data_directory/";
    }
    if (!directory.endsWith("/") && !directory.endsWith("\\")) {
        directory += "/";
    }

    let inputGtfs = {};  // key per GTFS file slug, value array of raw text content of GTFS file
    let inputOptions = {};  // geojson, config, callback

    try {
        const dir = fs.opendirSync(directory);
        let content;

        while ((content = dir.readSync()) !== null) {
            let fullName = content.name.toLowerCase();
            let extension = fullName.split(".").slice(-1)[0];
            let slug = fullName.split(".").slice(0, -1).join(".");

            // Logic below mirrors gtfs.js init.onLoad()
            if (extension === "json" || extension === "geojson") {
                let json = JSON.parse(fs.readFileSync(directory + fullName, "utf8"));
                if ("type" in json && json.type === "FeatureCollection") {
                    inputOptions["geojson"] = json;
                } else {
                    inputOptions["config"] = json;
                }
            } else if (extension === "txt") {  // Assume GTFS data
                if (slug !== "shapes") {  // Shapes is unused by GTFS to Aquius, but often large
                    inputGtfs[slug] = await readTxtToArray(directory + fullName);
                }
            }
        }

        dir.closeSync();

    } catch (err) {
        return "Failed when reading input data: " + err;
    }

    const result = processing(inputGtfs, inputOptions);

    if ("error" in result) {
        return "Failed with processing error: " + result.error.join(", ");
    }

    try {
        fs.writeFileSync(directory + "aquius.json", JSON.stringify(result.aquius), "utf8");
        fs.writeFileSync(directory + "config.auto", JSON.stringify(result.config, null, 2), "utf8");
        if ("networkTable" in result) {
            fs.writeFileSync(directory + "network.csv", tableToCsv(result.networkTable), "utf8");
        }
        if ("summaryTable" in result) {
            fs.writeFileSync(directory + "histogram.csv", tableToCsv(result.summaryTable), "utf8");
        }
    } catch (err) {
        return "Failed when writing output data: " + err;
    }

    let success = ["Success: Aquius written to '" + directory + "'."];
    if ("warning" in result) {
        success = success.concat(result.warning);
    }

    return success.join(". ");

}

function tableToCsv(table) {
    /**
     * Creates CSV block from table data object.
     * @param {object} table is a dict consisting data, format, header, each holding arrays,
     *  as generated by gtfs.process buildNetwork/SummaryTable() functions
     * @return {string} CSV block
     */

    let csv = sanitizeRow(table.header, []).join(",") + "\n";  // Headers are all text fields
    for (let i = 0; i < table.data.length; i += 1) {
        csv += sanitizeRow(table.data[i], table.format).join(",") + "\n";
    }
    return csv;
}

function sanitizeRow(row, format) {
    /**
     * Quotes text fields in row, as defined by format, or quoted where no format provided.
     * @param {Array} row - table row
     * @param {Array} format - table format, expected "Number" or "Text"
     * @return {Array} row with text sanitised in quotes
     */
    for (let i = 0; i < row.length; i += 1) {
        if (i > (format.length - 1) || format[i] === "Text") {
            // Crude quoting of text fields
            row[i] = '"' + String(row[i]).replace('"', "'") + '"';
        }
    }
    return row;
}

function readTxtToArray(filepath) {
    /**
     * Reads text file at filepath in an array of lines.
     * This structure is required to handle very long files,
     * where accumulated string could exceed 512MB buffer.
     * @param {string} filepath - .txt file to process
     * @return {Array} - array of string lines
     */
    return new Promise((resolve) => {
        let lines = [];
        let stream = fs.createReadStream(filepath);
        let reader = readline.createInterface({
            input: stream,
            output: process.stdout,
            terminal: false,
        });
        reader.on("line", function (line) {
            lines.push(line + "\n");
        });
        reader.on("close", function() {
            reader.close();
            resolve(lines);
        });
    });
}


(async () => {
    console.log(await init());
 })()
